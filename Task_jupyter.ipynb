{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "path = 'three_minutes_tweets.json.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как и говорил Вам, обожаю сайт: stackoverflow.com. https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data\n",
    "## Придумывать что-то своё я также люблю, не меньше :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7338\n",
      "7338\n",
      "7338\n",
      "7338\n",
      "7338\n",
      "7338\n",
      "7338\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "name = []\n",
    "tweet_text = []\n",
    "country_code = []\n",
    "display_url = []\n",
    "lang = []\n",
    "created_at = []\n",
    "location = []\n",
    "\n",
    "for line in open(path, 'r'):\n",
    "    tweets.append(json.loads(line))\n",
    "for i in range (1, len(tweets)):\n",
    "    \n",
    "    if 'delete' not in tweets[i]: \n",
    "#         print (i)\n",
    "        name.append (tweets[i]['user']['name'])\n",
    "        tweet_text.append (tweets[i]['text'])\n",
    "        lang.append (tweets[i]['lang'])\n",
    "        created_at.append(tweets[i]['created_at'])\n",
    "        location.append(tweets[i]['user']['location'])\n",
    "        if not tweets[i]['entities']['urls']:\n",
    "            display_url.append (\"None\")\n",
    "        else:\n",
    "            display_url.append (tweets[i]['entities']['urls'][0]['display_url'])\n",
    "        \n",
    "        if not tweets[i]['place']:\n",
    "            country_code.append (\"None\")\n",
    "        else:\n",
    "            country_code.append (tweets[i]['place'][\"country_code\"])\n",
    "\n",
    "print (len(name))\n",
    "print (len(tweet_text))\n",
    "print (len(country_code))\n",
    "print (len(lang))\n",
    "print (len(created_at))\n",
    "print (len(location))\n",
    "print (len(display_url))\n",
    "\n",
    "# print (name [:2])\n",
    " #['user'] ['screen_name'] #['created_at'] #такой подойдёт?\n",
    "# print ('delete' in tweets[29]) #0,29 -- False 102 - Russian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (tweets[117]['place'][\"country_code\"]) #ураа, в 117 \n",
    "# df1.loc[df1[\"country_code\"] != None]\n",
    "# df1.to_excel('sdf.xlsx')\n",
    "# print (type(tweets[3000]['place']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tweet_sentiment':tweet_sentiment\n",
    "dictio = {'name': name,\n",
    "                  'tweet_text': tweet_text,\n",
    "                  'country_code' : country_code,\n",
    "                  'lang':lang,\n",
    "                  'created_at': created_at,\n",
    "                  'location': location,\n",
    "                  'display_url': display_url,\n",
    "                  \n",
    "                 }\n",
    "df1 = pd.DataFrame.from_dict(dictio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentiment = \"AFINN-111.txt\"\n",
    "\n",
    "import csv\n",
    "word = []\n",
    "mark = []\n",
    "# создадим словарик для удобства\n",
    "with open(text_sentiment) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i in list(reader):\n",
    "        word.append(i[0])\n",
    "        mark.append(i[1])\n",
    "\n",
    "\n",
    "text_sentiment_dict = dict(zip(word, mark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "stroka = df1.loc[7319]['tweet_text']\n",
    "spisok = stroka.split ()\n",
    "tweet_sentiment_sum = 0\n",
    "for i in spisok:\n",
    "    if i in text_sentiment_dict:\n",
    "        print (i)\n",
    "        tweet_sentiment_sum += int(text_sentiment_dict[i])\n",
    "print (tweet_sentiment_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_it (string):\n",
    "#     spisok = string.split ()\n",
    "    tweet_sentiment_sum = 0\n",
    "    for i in string:\n",
    "        if i in text_sentiment_dict:\n",
    "            tweet_sentiment_sum += int(text_sentiment_dict[i])\n",
    "    return (tweet_sentiment_sum)\n",
    "\n",
    "text_sentiment_column = df1['tweet_text'].str.split(' ').apply(sentiment_it) \n",
    "df1[\"text_sentiment\"] = text_sentiment_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для генерации этого супер удобного файлика, используем:\n",
    "# pip freeze --local\n",
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLITE 3 WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "def insert_values(conn, data):\n",
    "    \"\"\"\n",
    "    Create a new project into the projects table\n",
    "    :param conn:\n",
    "    :param project:\n",
    "    :return: project id\n",
    "    \"\"\"\n",
    "    sql = u''' INSERT INTO twitter_table(name,tweet_text,country_code,lang,created_at, location, display_url, text_sentiment)\n",
    "              VALUES(?,?,?,?,?,?,?,?)'''\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, data)\n",
    "    return cur.lastrowid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "database = \"twitter.db\"\n",
    "conn = create_connection(database)\n",
    "with conn:\n",
    "    for index,row in df1.iterrows():\n",
    "        twitter_table = insert_values(conn, [row['name'], \n",
    "                                           row['tweet_text'], \n",
    "                                           row['country_code'], \n",
    "                                           row['lang'],\n",
    "                                           row['created_at'], \n",
    "                                           row['location'], \n",
    "                                           row['display_url'], \n",
    "                                           row['text_sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем запросы через SQLiteDatabaseBrowser -- всё норм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = u'Arabic (\\u0627\\u0644\\u0637\\u064a\\u0631\\u0627\\u0646)'\n",
    "print(txt)\n",
    "\n",
    "# .open twitter.db\n",
    "# delete from twitter_table;\n",
    "# select * from twitter_table;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
